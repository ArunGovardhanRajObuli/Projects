{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "465d0de1-4a85-45a5-ad06-a977ffbb034e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os, re, json, tempfile, traceback, textwrap, asyncio, nest_asyncio, shutil\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import whisper\n",
    "import pysrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 \n",
    "import clip\n",
    "import edge_tts\n",
    "from tqdm.auto import tqdm\n",
    "from whisper.utils import get_writer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "from __future__ import annotations\n",
    "import math, shlex, subprocess\n",
    "from typing import List\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips, concatenate_audioclips, AudioFileClip, vfx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c825565-8470-47aa-b846-c50fe51726ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT SETTINGS\n",
    "\n",
    "PROJECT_ROOT = Path(\"/home/jovyan\")\n",
    "MOVIE_TITLE  = \"Below the Deadline (1936)\"\n",
    "\n",
    "VIDEO_FILE = PROJECT_ROOT / \"ProjectVideos\" / f\"{MOVIE_TITLE}.mp4\"\n",
    "TRANS_DIR  = PROJECT_ROOT / \"transcripts\"\n",
    "\n",
    "VIDEO_STEM   = VIDEO_FILE.stem\n",
    "TXT_PATH     = TRANS_DIR / f\"{VIDEO_STEM}.txt\"\n",
    "SRT_PATH     = TRANS_DIR / f\"{VIDEO_STEM}.srt\"\n",
    "JSON_PATH = TRANS_DIR / f\"{VIDEO_STEM}.json\"\n",
    "MOVIE_SUM_TXT= PROJECT_ROOT / \"movie_summary.txt\"\n",
    "AUDIO_DIR = PROJECT_ROOT / \"narration\" / VIDEO_STEM.lower().replace(\" \", \"_\")\n",
    "AUDIO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_VIDEO    = PROJECT_ROOT / f\"{VIDEO_STEM.lower().replace(' ','_')}_reel.mp4\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1e9a4e-66f6-41f1-a1ee-a88852c9c80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Whisper] model=large-v3  device=cuda\n",
      "Transcript already exists - skipping.\n"
     ]
    }
   ],
   "source": [
    "# 1. TRANSCRIPTION\n",
    "\n",
    "model_size = \"large-v3\" # Selecting the large model for maximum accuracy.\n",
    "device     = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Setting device as the GPU by default.\n",
    "use_fp16   = device == \"cuda\"\n",
    "print(f\"[Whisper] model={model_size}  device={device}\")\n",
    "\n",
    "model = whisper.load_model(model_size, device=device) # We are loading the model here.\n",
    "TRANS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "if TXT_PATH.exists() and SRT_PATH.exists(): # Here, we check if the transcript for the film has already been generated so that we can mitigate redundancy.\n",
    "    print(\"Transcript already exists - skipping.\")\n",
    "else:\n",
    "    try:\n",
    "        res = model.transcribe(str(VIDEO_FILE), language=\"en\", fp16=use_fp16) # If the transcript is not generated we are transcribe the film.\n",
    "        TXT_PATH.write_text(res[\"text\"], encoding=\"utf-8\") # We are creating a text,SRT and JSON file of the transcript for future use.\n",
    "        get_writer(\"srt\", TRANS_DIR)(res, VIDEO_STEM)\n",
    "        with JSON_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(res, f, ensure_ascii=False, indent=2)\n",
    "        print(\"Transcription complete:\", SRT_PATH.name, JSON_PATH.name)\n",
    "    except Exception as e:\n",
    "        print(\"Transcription failed:\", e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "del model\n",
    "torch.cuda.empty_cache() # We are emptying the cache on the GPU as our summarization model requires 70 GB of VRAM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156b1cc4-add2-4134-b98c-987f2843d4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755626130.121164  128932 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755626130.129871  128932 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1755626130.244512  128932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755626130.244535  128932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755626130.244537  128932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1755626130.244539  128932 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "Loading checkpoint shards: 100%|██████████| 37/37 [08:09<00:00, 13.24s/it]\n"
     ]
    }
   ],
   "source": [
    "# 2. SUMMARY MODEL INITIALIZATION\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-72B-Instruct\" # After checking Mistral, Mixtral, llama and Yi, we found that Qwen gives us the best accuracy and has a 32k context window for our full transcript.\n",
    "bnb_cfg  = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ") # Configuring the model's settings in 4-bit precision to cut the memory usage while still keeping respectable precision.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True) # We load the tokenizer, allowing execution of any custom code in the repo.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            quantization_config=bnb_cfg\n",
    ") # Loading the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1377c9ac-76e1-415c-a732-fd1b401275e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2568"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. SUMMARY GENERATION\n",
    "\n",
    "transcript_text= TXT_PATH.read_text(encoding=\"utf-8\") # Initializing the transcript file to feed into the model.\n",
    "\n",
    "# The prompt created below gives a comprehensive and accurate summary of the transcript. \n",
    "\n",
    "prompt = textwrap.dedent(f\"\"\"\n",
    "### ROLE\n",
    "You are an professional film critic and narrative analyst.\n",
    "\n",
    "### GOAL\n",
    "Craft a *chronological* plot synopsis that:\n",
    "1. **Covers the five classic beats**  \n",
    "   • Setting & characters • Inciting incident • Rising action & turning points  \n",
    "   • Climax • Resolution  \n",
    "2. **Fits in exactly 20 sentences**, each on its own line.  \n",
    "3. Uses *present-tense, active voice*, vivid but precise language.  \n",
    "4. Contains **no dialogue quotes** and **no details not found in the transcript**.  \n",
    "5. Mentions each major character by name the first time they appear, then uses pronouns sparingly.\n",
    "\n",
    "### THOUGHT PROCESS  (*think step, hidden*)\n",
    "First: Read the transcript and silently extract  \n",
    "· main characters · time/place · inciting incident · major conflicts/turns · climax · resolution  \n",
    "Second: Outline those items in bullets (in your own mind).  \n",
    "**Do NOT reveal these notes.**\n",
    "Third: Reanalyze to check if you are hallucinating.Correct yourself if you are wrong.\n",
    "\n",
    "### OUTPUT\n",
    "Write the final **engaging** synopsis only, numbered 1-20, one sentence per line.  \n",
    "Do not write anything else—no title, no headings, no notes.\n",
    "\n",
    "### SOURCE MATERIAL\n",
    "The full transcript is below, delimited by triple angle brackets.  \n",
    "Use *only* facts that appear inside those brackets.\n",
    "\n",
    "<<<TRANSCRIPT>>>\n",
    "{transcript_text}\n",
    "<<<END OF TRANSCRIPT>>>\n",
    "\"\"\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=32768\n",
    ").to(model.device) # Feeding the prompt into the model for interpretation with max context length as 32k tokens, anything longer will be truncated.\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.1,\n",
    "    top_p= 0.9,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.15\n",
    ") # Generating outputs that have up to 1k tokens. \n",
    "\n",
    "input_length = inputs[\"input_ids\"].shape[-1] # We find out how many tokens were taken up by the prompt.\n",
    "generated_ids = outputs[0][input_length:] # Trimming off the prompt portion and keeping only the generated summary portion.\n",
    "\n",
    "decoded = tokenizer.decode(generated_ids, skip_special_tokens=True).strip() # Decoding the generated token IDs back into a string, skipping special tokens.\n",
    "\n",
    "summary_lines = re.findall(r\"^\\d+\\..*?$\", decoded, flags=re.MULTILINE) # Now, we use a regex to extract only lines starting with a number followed by a dot.\n",
    "\n",
    "if summary_lines:\n",
    "    summary_lines[-1] = re.sub(r\"\\s*###.*$\", \"\", summary_lines[-1]) # Here, we are removing all the extra headers and markers as we need only the sentences.\n",
    "\n",
    "summary = \"\\n\".join(summary_lines[:20]) # Now, we create the 20 numbered summary sentences.\n",
    "\n",
    "MOVIE_SUM_TXT.write_text(summary, encoding=\"utf-8\") # Writing the summary into into a file for future use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e400736-9a35-4384-a6fc-0b352998a409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence-level TTS complete: /home/jovyan/narration/below_the_deadline_(1936)\n"
     ]
    }
   ],
   "source": [
    "# 4. TEXT TO SPEECH\n",
    "\n",
    "async def _gen_tts(text: str, out_path: Path,\n",
    "                   voice=\"en-US-GuyNeural\",\n",
    "                   rate=\"+0%\", pitch=\"+0Hz\"):\n",
    "\n",
    "    communicator = edge_tts.Communicate(text, voice=voice, rate=rate, pitch=pitch) # Initializing a TTS communicator with the desired voice, speaking rate and pitch.\n",
    "    await communicator.save(str(out_path)) # Saving the generated audio.\n",
    "\n",
    "async def make_summary_tts(summary_path: Path = MOVIE_SUM_TXT,\n",
    "                           out_dir: Path = AUDIO_DIR) -> Path | None:\n",
    "\n",
    "    lines = [ln.strip() for ln in summary_path.read_text().splitlines() if ln.strip()] # Splitting the summary into lines, stripping whitespaces, and skipping empty lines if any.\n",
    "    sentences = [re.sub(r\"^\\d+\\.\\s*\", \"\", ln) for ln in lines] # Since our sentences start with numbering we remove the numbers to get only the sentences.\n",
    "\n",
    "    pad = len(str(len(sentences)))\n",
    "    tasks = []\n",
    "    for i, sent in enumerate(sentences, 1): # We are creating a TTS task for each sentences here.\n",
    "        fname = f\"sent_{i:0{pad}d}.mp3\" # Saving that file name as sent_01.mp3 and so on.\n",
    "        tasks.append(_gen_tts(sent, out_dir / fname)) # Creating the voiceover and writing it to the file.\n",
    "    await asyncio.gather(*tasks) # Running all the tasks concurrently.\n",
    "    print(\"Sentence-level TTS complete:\", out_dir)\n",
    "\n",
    "nest_asyncio.apply() # We need nested loops here or else the kernel is crashing.\n",
    "asyncio.run(make_summary_tts()) # Running the functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f68efa-be5e-4203-bdfe-84325b134767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/jovyan/ProjectVideos/Below the Deadline (1936)_clean.mp4')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLEANING VIDEO FILE\n",
    "\n",
    "# The video file needs a constant 24 FPS, H.264 video and AAC audio, so we re-encode the original file so it doesn't crash while building a reel using moviepy.\n",
    "\n",
    "TARGET_FPS  = 24.0\n",
    "\n",
    "def clean_video(src: Path, dst: Path, fps: float, crf: int = 17):\n",
    "    if dst.exists():\n",
    "        return dst\n",
    "    cmd = (\n",
    "        \"ffmpeg -y -fflags +genpts -err_detect ignore_err \"\n",
    "        f\"-i {shlex.quote(str(src))} \"\n",
    "        f\"-vf \\\"fps={fps},setpts=PTS-STARTPTS\\\" \"\n",
    "        f\"-map 0:v -map 0:a? -c:v libx264 -preset slow -crf {crf} \"\n",
    "        \"-pix_fmt yuv420p -c:a aac -b:a 160k -movflags +faststart \"\n",
    "        f\"{shlex.quote(str(dst))}\"\n",
    "    )\n",
    "    subprocess.run(cmd, shell=True, check=True)\n",
    "    return dst\n",
    "\n",
    "\n",
    "CLEAN_FILE = VIDEO_FILE.with_name(VIDEO_FILE.stem + \"_clean.mp4\")\n",
    "clean_video(VIDEO_FILE, CLEAN_FILE, TARGET_FPS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8130470-e797-4d23-82ce-89282640d6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences rewritten for CLIP\n",
      "4118 probe frames indexed\n",
      "Moviepy - Building video /home/jovyan/below_the_deadline_(1936)_reel.mp4.\n",
      "MoviePy - Writing audio in below_the_deadline_(1936)_reelTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video /home/jovyan/below_the_deadline_(1936)_reel.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/jovyan/below_the_deadline_(1936)_reel.mp4\n",
      "Reel exported → /home/jovyan/below_the_deadline_(1936)_reel.mp4\n"
     ]
    }
   ],
   "source": [
    "# 5. REEL BUILDING MAIN\n",
    "\n",
    "CROSS       = 0.5\n",
    "FRAME_STRIDE= 1.0\n",
    "CLIP_MODEL  = \"ViT-L/14\"\n",
    "USE_REWRITE = True\n",
    "REWRITE_TEMP= 0.3\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# We are loading the summary lines to convert it into visual cues for better clip matching.\n",
    "sent_lines = [ln.strip() for ln in MOVIE_SUM_TXT.read_text().splitlines() if ln.strip()]\n",
    "sent_text  = [re.sub(r\"^\\\\d+\\\\.\\\\s*\", \"\", ln) for ln in sent_lines]\n",
    "\n",
    "# Rewriting the summary sentences as visual sentences.\n",
    "if USE_REWRITE:\n",
    "    def rewrite(s):\n",
    "        prompt = f\"Rewrite for CLIP image search: '{s}'. Make it short, concrete, visual.\"\n",
    "        inp = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        out = model.generate(**inp, max_new_tokens=32, temperature=REWRITE_TEMP, do_sample=True)\n",
    "        return tokenizer.decode(out[0][inp.input_ids.shape[-1]:], skip_special_tokens=True).strip()\n",
    "    sent_text = [rewrite(s) for s in sent_text]\n",
    "    print(\"Sentences rewritten for CLIP\")\n",
    "\n",
    "# Now, we embed the sentences for visual matching.\n",
    "clip_model, clip_prep = clip.load(CLIP_MODEL, device=DEVICE) # Loading the CLIP model and preprocessing function.\n",
    "with torch.no_grad():\n",
    "    sent_emb = clip_model.encode_text(clip.tokenize(sent_text).to(DEVICE)).float() # Tokenizing and encoding the input sentence text into an embedding.\n",
    "    sent_emb /= sent_emb.norm(dim=-1, keepdim=True) # Normalizing the embedding to unit length.\n",
    "\n",
    "\n",
    "base_vid = VideoFileClip(str(CLEAN_FILE)).without_audio().set_fps(TARGET_FPS) # Here, we prepare the video file for frame extraction by removing audio and enforcing a fixed fps.\n",
    "MOV_END  = base_vid.duration # Finding the total duration of the film in seconds to prevent out of limit clip searching.\n",
    "EPS      = 1.0 / TARGET_FPS # This acts as a tiny one frame safety margin to prevent out of bounds seeking of clips.\n",
    "\n",
    "# Indexing video frames at regular intervals.\n",
    "probe_ts, probe_imgs = [], []\n",
    "cap = cv2.VideoCapture(str(CLEAN_FILE)) # Opening video file for frame capture.\n",
    "while True:\n",
    "    t = len(probe_ts) * FRAME_STRIDE # Next timestamp = index * stride.\n",
    "    if t >= MOV_END - 0.5: # Stopping when close to video end.\n",
    "        break\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, t * 1000) # Seeking to t seconds.\n",
    "    ok, frame = cap.read() # Reading the frame.\n",
    "    if not ok:\n",
    "        break\n",
    "    probe_ts.append(t) # Recording the timestamp.\n",
    "    probe_imgs.append(\n",
    "        clip_prep(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))).to(DEVICE)\n",
    "    ) # Pre processing and storing the frame.\n",
    "cap.release()\n",
    "\n",
    "# Batch-encoding all the probed video frames into CLIP embeddings for text‑to‑image similarity comparisons.\n",
    "with torch.no_grad():\n",
    "    frame_emb = []\n",
    "    for i in range(0, len(probe_imgs), 128):\n",
    "        batch = torch.stack(probe_imgs[i : i + 128]).half()\n",
    "        e = clip_model.encode_image(batch).float()\n",
    "        e /= e.norm(dim=-1, keepdim=True)\n",
    "        frame_emb.append(e.cpu())\n",
    "    frame_emb = torch.cat(frame_emb).to(DEVICE)\n",
    "\n",
    "print(f\"{len(probe_ts)} probe frames indexed\")\n",
    "\n",
    "# Now that we have a coarse timestamp, we scan window seconds in steps to find the frame whose embedding best matches the text embedding.\n",
    "def refine_pick(text_emb, coarse_t, window=15.0, step=0.25):\n",
    "    times, frames, ts = [], [], [] # Lists to collect candidate frames and their timestamps.\n",
    "    for t in np.arange(max(0, coarse_t - window), min(MOV_END, coarse_t + window), step): # Start at coarse_t - window and end at coarse_t + window.\n",
    "        frame_ok, fr = False, None\n",
    "        vcap = cv2.VideoCapture(str(CLEAN_FILE)) # Opening the video at time t (in milliseconds).\n",
    "        vcap.set(cv2.CAP_PROP_POS_MSEC, t * 1000) # Seeking to t seconds.\n",
    "        ok, fr = vcap.read() # Reading the frame.\n",
    "        vcap.release()\n",
    "        if not ok:\n",
    "            continue # Skipping the timestamp if frame couldn't be read.\n",
    "        frames.append(\n",
    "            clip_prep(Image.fromarray(cv2.cvtColor(fr, cv2.COLOR_BGR2RGB))).to(DEVICE)\n",
    "        ) # Collecting frames after converting BGR to RGB and preprocessing for CLIP.\n",
    "        ts.append(t) # Now we record this candidate timestamp.\n",
    "    if not frames:\n",
    "        return coarse_t # If we can't find any valid frames, we fall back to coarse timestamp.\n",
    "    with torch.no_grad():\n",
    "        f_emb = clip_model.encode_image(torch.stack(frames).half()).float()\n",
    "        f_emb /= f_emb.norm(dim=-1, keepdim=True)\n",
    "        best = torch.argmax(text_emb @ f_emb.T).item() # Computing the similarity between text embedding and frame embedding and returning the best match.\n",
    "    return ts[best]\n",
    "\n",
    "# Clip-selection loop.\n",
    "pad_len     = len(str(len(sent_text))) # Zero-pad width for filenames.\n",
    "narr_durs   = [\n",
    "    AudioFileClip(str(AUDIO_DIR / f\"sent_{i:0{pad_len}d}.mp3\")).duration\n",
    "    for i in range(1, len(sent_text) + 1)\n",
    "] # We are collecting the durations of each TTS audio file so that we can cut clips that match the narration length.\n",
    "BLOCK       = max(narr_durs) / 2 # Half-block to prevent overlap.\n",
    "\n",
    "# Now we define a function to compute a start/end window of length equal to the narration duration.\n",
    "def centred_slice(mid: float, dur: float) -> Tuple[float, float]:\n",
    "    st, en = mid - dur / 2, mid + dur / 2 # Initial half‑window around midpoint.\n",
    "    if st < 0:\n",
    "        en -= st; st = 0 # If start lesser than 0, we shift window forward to avoid crashes.\n",
    "    if en > MOV_END:\n",
    "        shift = en - MOV_END; st = max(0, st - shift); en = MOV_END # If end is greater than video length, we shift window back.\n",
    "    en = min(en, MOV_END - EPS) # Subtracting EPS to avoid slicing at exact end.\n",
    "    return st, en # Returning valid (start, end) times.\n",
    "\n",
    "used_windows: list[Tuple[float, float]] = [] # List of already selected time windows, this is used to prevent overlap.\n",
    "chosen_mid:  list[float] = [] # List of midpoints of chosen segments.\n",
    "last_t = -1.0 # Initializing last chosen timestamp.\n",
    "\n",
    "# Now, we loop through each sentence embedding and its narration duration.\n",
    "for idx, (emb, dur) in enumerate(zip(sent_emb, narr_durs), 1):\n",
    "    sims  = emb @ frame_emb.T # We compute similarity scores of all frame embeddings.\n",
    "    order = sims.argsort(descending=True).tolist() # Here, we rank frame indices by descending similarity.\n",
    "\n",
    "# For maintaining unique clips we check if the current probed timestamp is far enough from used windows.\n",
    "    def probe_ok(j):\n",
    "        t = probe_ts[j]\n",
    "        return all(not (st - BLOCK <= t <= en + BLOCK) for st, en in used_windows)\n",
    "\n",
    "    eligible = [j for j in order if probe_ok(j)] or order\n",
    "\n",
    "    for pick in eligible:\n",
    "        mid   = refine_pick(emb, probe_ts[pick]) # Now, we refine the coarse timestamp to best match.\n",
    "        st, en = centred_slice(mid, dur) # Computing the slice around refined timestamp.\n",
    "        if any(os < en and st < oe for os, oe in used_windows):\n",
    "            continue # Skipping if overlapping any used window.\n",
    "        chosen_mid.append(mid) # Accepting selected midpoint.\n",
    "        used_windows.append((st, en)) # Marking its window as used.\n",
    "        last_t = mid # Updating last chosen time.\n",
    "        break # Moving on to next sentence.\n",
    "\n",
    "# Building the reel.\n",
    "clips = [] # List for all the subclips of each sentence.\n",
    "for i, mid in enumerate(chosen_mid, 1):\n",
    "    narr = AudioFileClip(str(AUDIO_DIR / f\"sent_{i:0{pad_len}d}.mp3\")) # Loading the the TTS audio. \n",
    "    st, en = centred_slice(mid, narr.duration) # Computing the video slice.\n",
    "    clip_v = base_vid.subclip(st, en).set_audio(narr) # Attaching the audio.\n",
    "    if i > 1:\n",
    "        clip_v = clip_v.crossfadein(CROSS) # Adding crossfade between clips.\n",
    "    clips.append(clip_v) # Appending the clip to the list.\n",
    "\n",
    "# Finally, we export the reel.\n",
    "final = concatenate_videoclips(clips, method=\"compose\", padding=-CROSS).set_fps(TARGET_FPS) # Concatenating the clips at the constant 24 FPS.\n",
    "final.write_videofile(\n",
    "    str(OUT_VIDEO),\n",
    "    fps=TARGET_FPS,\n",
    "    codec=\"libx264\",\n",
    "    audio_codec=\"aac\",\n",
    "    preset=\"medium\",\n",
    "    bitrate=\"4000k\",\n",
    "    ffmpeg_params=[\"-err_detect\", \"ignore_err\"],\n",
    ")\n",
    "print(\"Reel exported →\", OUT_VIDEO.resolve())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266841f3-c096-4782-a7e1-6b5a899f2872",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
