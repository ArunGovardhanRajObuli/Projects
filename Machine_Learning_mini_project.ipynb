{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArunGovardhanRajObuli/Projects/blob/main/Machine_Learning_mini_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TuOHlqE-eOm"
      },
      "source": [
        "#**1 Author**\n",
        "\n",
        " Student Name: Arun Govardhan Raj Obuli\n",
        "\n",
        " Student ID: 241024544"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtBETsj1-YiC"
      },
      "source": [
        "# **2 Problem formulation**\n",
        "\n",
        "The machine learning problem that we are trying to solve here is to identify deceptive stories. Our aim is to classify audio recordings of stories as deceptive or truthful based on features such as pitch, loudness, Mel-Frequency Cepstral Coefficients (MFCCs), Zero Crossing Rate (ZCR), spectral centroid, and spectral bandwidth.\n",
        "\n",
        "This is an intriguing problem because deceptive speech often exhibits subtle variations in these audio features, which can be captured and analyzed. Developing a good deception detection model can be useful in fields like law enforcement, where it could assist in detecting lies during interrogations, and in psychological research, where understanding deception can provide valuable insights into human behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4DYw_k9A7Kg"
      },
      "source": [
        "#**3 Methodology**\n",
        "\n",
        " **Training task**\n",
        "\n",
        "First, we extract the relevant features such as pitch, loudness, Mel-Frequency Cepstral Coefficients, Zero crossing rate, spectral centroid and bandwidth from the audio recordings. These features correlate strongly with deceptive speech. Next, we preprocess the features by standardization and dimensionality reduction.\n",
        "\n",
        "We then train a Support Vector Machine (SVM) model using the preprocessed features. The SVM is configured with class_weight='balanced' to handle class imbalance and ensure fair learning across both classes.\n",
        "\n",
        "**Test task**\n",
        "\n",
        "The test task is performed on an independent test set, which is completely separate from the training dataset to ensure model generalization. The test set is created by splitting the dataset using GroupShuffleSplit, ensuring no overlap of stories between the training and test sets. This prevents data leakage, where information from the test set could influence the training process.\n",
        "\n",
        "**Performance Metrics**\n",
        "\n",
        "1. Accuracy - This provides the percentage of correct predictions.\n",
        "2. Classification Report - Gives the precision, recall, F1-score, and support for each class.\n",
        "\n",
        "**Additional tasks**\n",
        "\n",
        "1. We split the audio files into overlapping 30 second segments.\n",
        "2. Using Groupshufflesplit to prevent data leakage(i.e. to make sure the segments of the same story does not appear in the training and testing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm41iBSlBotX"
      },
      "source": [
        "# **4 Implemented ML prediction pipelines**\n",
        "\n",
        "**Overview**\n",
        "\n",
        "We take in the raw audio files and labels and perform segmentation, feature extraction, standardization and dimensionality reduction and train/test split making sure the segments of the same story are either in the training or test set. We then train an SVM classifier with class weight balancing for imbalanced data. This trained model is then used to make predictions on the test data and calculate the accuracy, precision, recall, and F1-score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwwny0Exk1D5"
      },
      "source": [
        "# 4.1 Transformation stage\n",
        "\n",
        "**Input**: Audio file path(List of strings) and labels(Pandas DataFrame).\n",
        "\n",
        "In this stage, we load the audio files at the given sampling rate and check if its above the min duration. Then we calculate the length of each segment and the step size based on the overlap percentage and generate the segements using the sliding window approach. Each segment is 30 seconds long and overlaps with the previous segment by 50%. This ensures that the entire audio is captured without cutting out segments that are lesser than the segment duration. It also helps enhance the feature extraction as overlapping segments can capture subtle shifts in spectral features or energy that might be missed in non-overlapping segments.\n",
        "\n",
        "We are extracting the features from the audio files as this decreases the complexity and reduces the noise when compared to the raw audio files.\n",
        "\n",
        "1. **Pitch** : Pitch has been choosen as one of the features here as higher pitch may be an indicator of deception. If the mean pitch of the speaker is significantly higher than the baseline pitch, this might indicate that the speaker is stressed and must be probably lying. High variability in the pitch is an indicator of nervousness and can be related to lying hence we find this variability through calculating the standard deviation.\n",
        "\n",
        "2. **Intensity** : Higher vocal intensity(loudness) may be an indicator of deception hence we are extracting the RMS value of the audio.\n",
        "\n",
        "3. **MFCC** : Deceptive speech can cause changes in vocal tract configuration due to stress or anxiety. Mel-Frequency Cepstral Coefficients (MFCCs) can be useful in representing these changes by modelling the frequency content of the speech. We calcuate the MFCC mean and standard deviation in each segement to provide a summary of the segment's spectral shape and variability in the spectral content over time.\n",
        "\n",
        "4. **Zero crossing rate** : Zero crossing rate can provide insights into frequency content. We are calculating the mean of the ZCR, higher mean suggests stress, nervousness, or unintentional articulation changes during deception.\n",
        "\n",
        "5. **Spectral Centroid and bandwidth** : We are also extracting spectral centroid and bandwidth which can help capture details related to energy distribution of the audio, which could change due to emotional states associated with deception.\n",
        "\n",
        "All of the above features are extracted for all segments and appended to a feature vector. The same is done for labels and story Id's.\n",
        "\n",
        "This feature matrix is taken as input to a standard scaler which standardizes the features so that all features contribute equally to the analysis and also helps in handling the different feature units. PCA is then applied on this standardized feature matrix to reduce its dimensionality and identify the principal components that capture the maximum variance in the data. PCA will help prevent the model from overfitting.\n",
        "\n",
        "**Output**: A standardized, reduced-dimension feature matrix.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnKineMKnoFf"
      },
      "source": [
        "# 4.2 Model stage\n",
        "\n",
        "**Input**: Transformed Feature matrix from the transformation stage.\n",
        "\n",
        "\n",
        "We have chosen the SVM model for this pipeline after trying the other models such as random forest and gradient boosting. The other models tend to memorize the training data and overfit due to their high complexity. Here, we are choosing the SVM classifier with the rbf(Radial Basis Function) kernel which is good at handling non-linear relationships as it helps the SVM find a non-linear decision boundary that seperates the classes effectively. Since the feature matrix is high dimensional, SVM's can handle them better as they focus on support vectors rather than the entire dataset. By focusing only on support vectors, SVMs with an RBF kernel are less influenced by outliers compared to models like k-Nearest Neighbors or linear regression. SVM's allow class weighting, which can help in representing the minority class better. Hence, we have chosen the SVM with rbf kernel as our model in this project.\n",
        "\n",
        "**Output**: Predictions for the data and evaluation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3OEWT4Tue47"
      },
      "source": [
        "# 5 Dataset\n",
        "We are using training and test datasets to evaluate our model.\n",
        "\n",
        "Since we are segmenting the audio files into multiple segments and the segments of the same audio are to be kept together in either the training set or the test set to prevent data leakage, we use Groupshufflesplit. If the data is split randomly, some of the information from the test set might influence the training set, leading to inaccurate performance metrics. To ensure class balance we are using class_weights= 'balanced' in the SVM to ensure there is equal representation of both classes.\n",
        "\n",
        "**Training Dataset**\n",
        "\n",
        "We use the grouped dataset from GroupShuffleSplit to train our model. The training dataset consists of 80% of the MLEnd dataset, ensuring that segments from the same audio file remain within the training set. The training set is used to fit the model and learn meaningful patterns from the data.\n",
        "\n",
        "**Test Dataset**\n",
        "\n",
        "We use this dataset to evaluate the performance of our model. The test set consists of the remaining 20% of the MLEnd dataset, ensuring it is completely independent of the training dataset. This guarantees that the test set provides an unbiased evaluation of the model’s generalization ability.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "1. There is a dependance on labels, we have assumed that the labels are accurate while balancing the classes. If the labels are incorrect this will amplify the negative impact while balancing.\n",
        "\n",
        "2. The balancing might work well with the training data but might fail to generalize unseen test data, especially when the test data has imbalanced distribution.\n",
        "\n",
        "3. During segmentation we have chosen to overlap the segments to capture the entire audio file, this overlap might lead to redundancy as adjacent segments might share similar features.\n",
        "\n",
        "4. There is an imbalance in the representation between the classes and this might lead to the model overfitting to that class.\n",
        "\n",
        "5. As the dataset size is small, it is not enough to train a complex model. This limited data might lead to overfitting."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlend==1.0.0.4\n",
        "\n",
        "import mlend\n",
        "from mlend import download_deception_small, deception_small_load\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "import librosa\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=ImportWarning)\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "datadir = download_deception_small(save_to='/content/drive/MyDrive/Data/MLEndDeception', subset={}, verbose=1, overwrite=False)\n",
        "audio_files = glob.glob('/content/drive/MyDrive/Data/MLEndDeception/deception/MLEndDD_stories_small/*')\n",
        "labels_file = pd.read_csv('MLEndDD_story_attributes_small.csv').set_index('filename')"
      ],
      "metadata": {
        "id": "Dm19jxzANY2J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ae8d0c-e1d0-4fed-cb07-6ca403784c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mlend==1.0.0.4 in /usr/local/lib/python3.10/dist-packages (1.0.0.4)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from mlend==1.0.0.4) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mlend==1.0.0.4) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlend==1.0.0.4) (1.13.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mlend==1.0.0.4) (3.8.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from mlend==1.0.0.4) (1.4.2)\n",
            "Requirement already satisfied: spkit in /usr/local/lib/python3.10/dist-packages (from mlend==1.0.0.4) (0.0.9.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mlend==1.0.0.4) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mlend==1.0.0.4) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mlend==1.0.0.4) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mlend==1.0.0.4) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mlend==1.0.0.4) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mlend==1.0.0.4) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mlend==1.0.0.4) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mlend==1.0.0.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mlend==1.0.0.4) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->mlend==1.0.0.4) (2024.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from spkit->mlend==1.0.0.4) (1.5.2)\n",
            "Requirement already satisfied: python-picard in /usr/local/lib/python3.10/dist-packages (from spkit->mlend==1.0.0.4) (0.8)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from spkit->mlend==1.0.0.4) (1.8.0)\n",
            "Requirement already satisfied: pylfsr in /usr/local/lib/python3.10/dist-packages (from spkit->mlend==1.0.0.4) (1.0.7)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from spkit->mlend==1.0.0.4) (3.12.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from spkit->mlend==1.0.0.4) (0.13.2)\n",
            "Requirement already satisfied: phyaat in /usr/local/lib/python3.10/dist-packages (from spkit->mlend==1.0.0.4) (0.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mlend==1.0.0.4) (1.16.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->spkit->mlend==1.0.0.4) (3.5.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Downloading 100 stories (audio files) from https://github.com/MLEndDatasets/Deception\n",
            "100%|\u001b[92m▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓▓\u001b[0m|100\\100|00100.wav\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformation Stage\n",
        "\n",
        "def split_audio(file_path, seg_dur=30, sr=22050, overlap=0.5, min_dur=30):\n",
        "    x, fs = librosa.load(file_path, sr=sr)\n",
        "    if len(x) < min_dur * sr: # If the length of x at a given sampling rate is lesser than the minimum duration we skip the file.\n",
        "        print(f\"Skipping {file_path}: File too short ({len(x) / sr:.2f} seconds)\")\n",
        "        return [], fs\n",
        "    seg_len = int(seg_dur * sr) # The segment duration is 30 seconds as defined in the lecture. Segment length calculates the number of samples per segment\n",
        "    step_size = int(seg_len * (1 - overlap)) # The number of samples to move forward in the audio signal for each segment when splitting it into overlapping segments.\n",
        "    segments = [x[start:start + seg_len]\n",
        "                for start in range(0, len(x) - seg_len + 1, step_size)] # For start in range begining at 0 and incrementing by step_size until there is enough remaining data to extract a segment of size seg_len, we slice the original signal from index start to start+seg_len.\n",
        "    return segments, fs\n",
        "\n",
        "def getPitch(x, fs, winLen=0.02): # Input audio(x), sampling rate of the audio signal and the window length for pitch detection. 0.02 seconds is the default size for speech samples.\n",
        "    frame_length = int(winLen * fs) # Calculating the number of samples in one frame.\n",
        "    hop_length = frame_length // 2 # Defining the number of samples by which the window is shifted for the next frame.\n",
        "    f0, voiced_flag, _ = librosa.pyin(y=x, fmin=80, fmax=450, sr=fs, frame_length=frame_length, hop_length=hop_length) # Finding the fundamental frequency for each frame and determining if its voiced or unvoiced. Fmin and fmax are the min and max pitch expected for human speech.\n",
        "    return f0, voiced_flag\n",
        "\n",
        "def getIntensity(x):\n",
        "    return np.mean(librosa.feature.rms(y=x)) # Calculating average RMS energy across all frames in the audio signal, representing the overall intensity of the audio.\n",
        "\n",
        "def getMFCC(x, fs, n_mfcc=13): #n_mfcc is the number of MFCC coeffecients to extract\n",
        "    mfcc = librosa.feature.mfcc(y=x, sr=fs, n_mfcc=n_mfcc) # We extract the mfcc features.\n",
        "    return np.mean(mfcc, axis=1), np.std(mfcc, axis=1) # Calculating the mean and standard deviation of the features and representing them as 1D arrays.\n",
        "\n",
        "def getZeroCrossingRate(x):\n",
        "    return np.mean(librosa.feature.zero_crossing_rate(y=x)) # We extract the ZCR and calculate it's mean.\n",
        "\n",
        "def getSpectralFeatures(x, fs):\n",
        "    spectral_centroid = librosa.feature.spectral_centroid(y=x, sr=fs) # We extract the spectral centroid.\n",
        "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=x, sr=fs) # We extract the spectral bandwidth.\n",
        "    return np.mean(spectral_centroid), np.std(spectral_centroid), np.mean(spectral_bandwidth), np.std(spectral_bandwidth) # Calculate the mean and standard deviation of both the features.\n",
        "\n",
        "# Function to call the above functions and calculate the different features for each and every segment.\n",
        "def getXy_multiple_segments(files, labels_file, seg_dur=30, overlap=0.5, min_dur=30, scale_audio=True):\n",
        "    X, y, story_ids = [], [], [] # Initializing the outputs. X will store the list of feature vectors for all segments, y stores corresponding labels and story_ids store the file ID to track which file the segment came from.\n",
        "    for file in tqdm(files): # Itirating through the files\n",
        "        fileID = file.split('/')[-1]\n",
        "        if fileID not in labels_file.index: # Extracting the file ID and checking if it exists in labels file.\n",
        "            print(f\"Skipping {fileID}: Not found in labels file\") # Skipping if not present in labels file.\n",
        "            continue\n",
        "        yi = labels_file.loc[fileID, 'Story_type'] == 'true_story' # We retrieve the label for a file and convert it to a boolean value (True/false)\n",
        "        try:\n",
        "            segments, fs = split_audio(file, seg_dur=seg_dur, overlap=overlap, min_dur=min_dur) # We split the audio into overlaping segments by calling the split_audio function.\n",
        "            for segment in segments:\n",
        "                if scale_audio:\n",
        "                    segment = segment / np.max(np.abs(segment)) # We itirate through the segment and normalise it to scale it to the rage [-1,1].\n",
        "                # Extracting features\n",
        "                f0, voiced_flag = getPitch(segment, fs, winLen=0.02)\n",
        "                pitch_mean = np.nanmean(f0) if np.mean(np.isnan(f0)) < 1 else 0\n",
        "                pitch_std = np.nanstd(f0) if np.mean(np.isnan(f0)) < 1 else 0\n",
        "                voiced_fr = np.mean(voiced_flag) if voiced_flag is not None else 0\n",
        "                intensity = getIntensity(segment)\n",
        "                mfcc_mean, mfcc_std = getMFCC(segment, fs)\n",
        "                spectral_centroid_mean, spectral_centroid_std, spectral_bandwidth_mean, spectral_bandwidth_std = getSpectralFeatures(segment, fs)\n",
        "                zero_crossing_rate = getZeroCrossingRate(segment)\n",
        "\n",
        "                xi = [pitch_mean, pitch_std, voiced_fr, intensity,\n",
        "                      *mfcc_mean, *mfcc_std,\n",
        "                      spectral_centroid_mean, spectral_centroid_std, spectral_bandwidth_mean, spectral_bandwidth_std,\n",
        "                      zero_crossing_rate] # We build the feature vector here by combining all the extracted features for the segment.\n",
        "\n",
        "                if any(np.isnan(xi)) or any(np.isinf(xi)): # Skipping segment if any feature is zero or infinite.\n",
        "                    continue\n",
        "                X.append(xi) # We append the feature vector to the main list.\n",
        "                y.append(yi) # We append the labels to the main list.\n",
        "                story_ids.append(fileID) # We append the story_ids to the main list.\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {fileID}: {e}\")\n",
        "    return np.array(X), np.array(y), story_ids\n",
        "\n",
        "def transform_features(X, n_components=10):\n",
        "    # Standardizing the features.\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Applying PCA for dimensionality reduction.\n",
        "    pca = PCA(n_components=n_components, random_state=42)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    return X_pca, scaler, pca  # Return fitted scaler and PCA for use on test data."
      ],
      "metadata": {
        "id": "XfHxvnL6FTgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Stage\n",
        "\n",
        "def split_dataset(X, y, story_ids, test_size=0.2, random_state=42):\n",
        "    gss = GroupShuffleSplit(test_size=test_size, n_splits=1, random_state=random_state) # Defining the groupshufflesplit with test size as 20%, n_splits defines the number of reshuffling and splitting iterations. Here, it is 1.\n",
        "    for train_idx, test_idx in gss.split(X, y, groups=story_ids): # We are generating the indices for the training testing sets while respecting the grouping defined by story_ids.\n",
        "        X_train, X_test = X[train_idx], X[test_idx] # We split the feature matrix into training and testing subsets using the indices generated by Groupeshufflesplit.\n",
        "        y_train, y_test = y[train_idx], y[test_idx] # We split the labels into training and testing subsets using the indices generated by Groupeshufflesplit.\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "lz7OaNJPFa61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Stage\n",
        "\n",
        "def train_model(X_train, y_train):\n",
        "    svm_model = SVC(kernel='rbf', class_weight='balanced', random_state=42) # Defining the SVM with rbf kernel and to balance the classes.\n",
        "    svm_model.fit(X_train, y_train) # We fit the training data into the model.\n",
        "    return svm_model\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, X_train, y_train):\n",
        "    y_train_pred = model.predict(X_train) # Performing prediction on whether the story is true or deceptive the training data.\n",
        "    train_accuracy = accuracy_score(y_train, y_train_pred) # Calculating the training accuracy.\n",
        "    y_pred = model.predict(X_test) # Performing prediction on the test data.\n",
        "    accuracy = accuracy_score(y_test, y_pred) # Calculating the test accuracy.\n",
        "    report = classification_report(y_test, y_pred) # Calculating the performance metrics.\n",
        "    return train_accuracy, accuracy, report\n"
      ],
      "metadata": {
        "id": "GYbGe_02Fjxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 Experiments and results"
      ],
      "metadata": {
        "id": "KzOecZtdW6JP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extraction\n",
        "X, y, story_ids = getXy_multiple_segments(audio_files, labels_file)\n",
        "\n",
        "# Transformation\n",
        "X_transformed, scaler, pca = transform_features(X, n_components=6) # We have tried different n_components and have determined that 6 principal components prevent the model from overfitting to the training data and improve the test accuracy.\n",
        "\n",
        "# Dataset splitting\n",
        "X_train, X_test, y_train, y_test = split_dataset(X_transformed, y, story_ids)\n",
        "\n",
        "# Model training\n",
        "svm_model = train_model(X_train, y_train)\n",
        "\n",
        "# Model evaluation\n",
        "train_accuracy, accuracy, report = evaluate_model(svm_model, X_test, y_test, X_train, y_train)\n",
        "\n",
        "# Print results\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Classification Report:\\n{report}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzH9uMXHdDzE",
        "outputId": "98c5bffc-a71a-4c3e-8c1d-15ea20caedb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [1:05:02<00:00, 39.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.8208\n",
            "Test Accuracy: 0.7532\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.80      0.85      0.82       105\n",
            "        True       0.63      0.55      0.59        49\n",
            "\n",
            "    accuracy                           0.75       154\n",
            "   macro avg       0.71      0.70      0.71       154\n",
            "weighted avg       0.75      0.75      0.75       154\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training accuracy**:\n",
        "\n",
        "The model has a training accuracy of 82.08% which indicates that it performs well on the training data and has learned meaningful patterns.\n",
        "\n",
        "**Test accuracy**:\n",
        "\n",
        "The model has a test accuracy of 75.32% which indicates that it is performing relatively well on the test data.\n",
        "\n",
        "**Precision**:\n",
        "\n",
        "The model predicts 80% of the deceptive stories as deceptive as indicated by precision for the false class.\n",
        "\n",
        "The model predicts 63% of the true stories as true as indicated by the precision for true class. This suggests that the model is less confident in distinguishing true stories.\n",
        "\n",
        "**Recall**:\n",
        "\n",
        "The model predicts 85% of the actual deceptive stories as deceptive as indicated by the recall for false class.\n",
        "\n",
        "The model predicts only 55% of the actual true stories as true, indicating that the model has more false negatives.\n",
        "\n",
        "**F1 Score**:\n",
        "\n",
        "High F1-score of 82% indicates balanced performance for the false class.\n",
        "\n",
        "Lower F1-score of 59% indicates that the model is finding it difficult to predict the true class.\n",
        "\n",
        "**Macro Average**:\n",
        "\n",
        "The macro average give the majority and minority class the same weights hence a poor performance on the minority class will result in lower macro average score.The model struggles more with distinguishing the true class as indicated by the precision of 71%, recall of 70% and F1 score of 71%.\n",
        "\n",
        "**Weighted Average**:\n",
        "\n",
        "The weighted average gives a realistic estimate of the model's performance on the dataset by weighting metrics according to the number of samples in each class. Since the majority class has more weight it ca mask the poor performance of the minority class. The model's overall performance is slightly inflated by the larger deceptive class with a precision,recall and F1 score of 75%."
      ],
      "metadata": {
        "id": "kN0bL0GE2uwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 Conclusions\n",
        "\n",
        "In this project, we have addressed the problem of detecting deception from audio samples. Using the SVM model, we achieved an accuracy of 75% on the test dataset. While this is a promising baseline, there is still room for improvement:\n",
        "\n",
        "1. The dataset is limited, which causes the model to overfit the training data. This issue can be mitigated by collecting more labeled audio samples to improve model generalization.\n",
        "\n",
        "2. Not all extracted features may be equally important for detecting deception. By performing feature selection, we can focus on features with strong correlations to the true or deceptive labels and eliminate noise.\n",
        "\n",
        "3. While SVM performed well, exploring more advanced models like deep learning (e.g., CNNs or RNNs) or ensemble models (e.g., XGBoost, LightGBM) could improve accuracy by capturing more complex patterns in the data.\n",
        "\n",
        "4. Our feature extraction was done without deep domain expertise. Collaborating with domain experts in psychology or linguistics could help identify new features strongly correlated with deception, further refining the model.\n",
        "\n",
        "In conclusion, we have built a solid foundation for detecting deception using audio samples. By collecting more data, selecting the most relevant features, and exploring more advanced models with expert guidance, we can improve the accuracy and robustness of this approach."
      ],
      "metadata": {
        "id": "L72bm70DZpd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 References\n",
        "\n",
        "1. Streeter, L. A., Krauss, R. M., Geller, V., Olson, C., & Apple, W. (1977). Pitch changes during attempted deception. Journal of Personality and Social Psychology, 35(5), 345–350. https://doi.org/10.1037/0022-3514.35.5.345\n",
        "\n",
        "2. Rockwell, P., Buller, D. B., & Burgoon, J. K. (1997). The voice of deceit: Refining and expanding vocal cues to deception. Communication Research Reports, 14(4), 451–459.\n",
        "\n",
        "3. Anolli, L., & Ciceri, R. (1997). The voice of deception: Vocal strategies of naive and able liars. Journal of Nonverbal Behavior, 21(4), 259–284.\n",
        "\n",
        "4. Levitan, S. I., Maredia, A., & Hirschberg, J. (2018). Acoustic-prosodic indicators of deception and trust in interview dialogues. Proceedings of Interspeech 2018, 416–420. https://doi.org/10.21437/Interspeech.2018-1214\n",
        "\n",
        "5. Fan, C., Zhang, X., Wang, Y., & Wang, Y. (2015). Deceptive speech detection based on sparse representation. 2015 IEEE International Conference on Signal Processing, Communications and Computing (ICSPCC), 1–5. https://doi.org/10.1109/ICSPCC.2015.7515793"
      ],
      "metadata": {
        "id": "WbgBsxDHZ51a"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}